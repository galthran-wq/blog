[
  {
    "objectID": "posts/2025-09-25-learn-autogressive-model-on-images/index.html",
    "href": "posts/2025-09-25-learn-autogressive-model-on-images/index.html",
    "title": "Learn Autogressive Model on Images",
    "section": "",
    "text": "Wouldn’t it be nice to learn a distribution \\(P(X)\\) of a certain set of images, for simplicity, the images of digits?\nYou would be able to:\n\nGenerate new images of digits\nSay how like an image is from the same distribution (that is, also a digit)\ninpaint missing image details\n\nIt does not seem like a trivial task.\nAssume image is \\(\\sqrt{n} \\times \\sqrt{n}\\) pixels, each one black or white; enumerate them from the top left to bottom right \\(\\{ X_1, ..., X_{n}\\}\\)\nIf we make no assumption about the underlaying distribution, it would take \\(2^n-1\\) parameters to model it, since each pixel in this image can be either black or white with a certain probability \\(p_i\\)."
  },
  {
    "objectID": "posts/2025-09-25-learn-autogressive-model-on-images/index.html#the-problem",
    "href": "posts/2025-09-25-learn-autogressive-model-on-images/index.html#the-problem",
    "title": "Learn Autogressive Model on Images",
    "section": "",
    "text": "Wouldn’t it be nice to learn a distribution \\(P(X)\\) of a certain set of images, for simplicity, the images of digits?\nYou would be able to:\n\nGenerate new images of digits\nSay how like an image is from the same distribution (that is, also a digit)\ninpaint missing image details\n\nIt does not seem like a trivial task.\nAssume image is \\(\\sqrt{n} \\times \\sqrt{n}\\) pixels, each one black or white; enumerate them from the top left to bottom right \\(\\{ X_1, ..., X_{n}\\}\\)\nIf we make no assumption about the underlaying distribution, it would take \\(2^n-1\\) parameters to model it, since each pixel in this image can be either black or white with a certain probability \\(p_i\\)."
  },
  {
    "objectID": "posts/2025-09-25-learn-autogressive-model-on-images/index.html#a-solution",
    "href": "posts/2025-09-25-learn-autogressive-model-on-images/index.html#a-solution",
    "title": "Learn Autogressive Model on Images",
    "section": "A solution",
    "text": "A solution\nA common trick is to use the chain rule\n\\[P(X_1, ..., X_n) = P(X_1) P(X_2|X_1) ... P(X_n|X_1,X_2,...,X_{n-1})\\]\nand then, in a sort of “discriminative manner” to model\n\\[P(X_i|X_1...X_{i-1}) = f_i(X_1, ..., X_{i-1}; \\theta_i)\\]\nThis would be an example of an autoregressive model.\nNote that the function \\(f_i\\) and the set of parameters for this function \\(\\theta_i\\) might be different for each of those terms.\nSome easy example would be to just let\n\\[P(X_i|X_1...X_{i-1}) = \\sigma(\\theta_{i1} X_1 + \\theta_{i2} X_2 + ... + \\theta_{i,i-1} X_{i-1} + \\theta_{i0})\\]\nThis is essentially a separate logistic regression for each pixel, conditioned on all previous pixels.\nThe number of parameters in this model is \\(\\frac{n^2}{2}\\).\nA bit better seems to have just a dense representation for the previous pixels with a linear linear like\n\\[P(X_i|X_1...X_{i-1}) = \\sigma(\\theta_i^T h(W_{:,1} X_1 + W_{:,2} X_2 + ... + W_{:,i-1} X_{i-1})) = \\sigma(\\theta_i^T h(W X_{1:i-1}))\\]\nIn this case we are still taking a linear combination, but now of a hidden state of fixed size. The number of parameters in this model is \\(2DH + D + H\\), which scales linearly with the number of pixels in the image."
  },
  {
    "objectID": "posts/2025-09-25-learn-autogressive-model-on-images/index.html#a-demo",
    "href": "posts/2025-09-25-learn-autogressive-model-on-images/index.html#a-demo",
    "title": "Learn Autogressive Model on Images",
    "section": "A demo",
    "text": "A demo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is interesting"
  },
  {
    "objectID": "posts/2025-09-25-learn-autogressive-model-on-images/index.html#nade",
    "href": "posts/2025-09-25-learn-autogressive-model-on-images/index.html#nade",
    "title": "Learn Autogressive Model on Images",
    "section": "NADE",
    "text": "NADE"
  },
  {
    "objectID": "posts/2025-09-25-learn-autogressive-model-on-images/index.html#limitations",
    "href": "posts/2025-09-25-learn-autogressive-model-on-images/index.html#limitations",
    "title": "Learn Autogressive Model on Images",
    "section": "Limitations",
    "text": "Limitations\nWhat the autoregressive model cannot give us:\n\nGet a representation of an image.\n\nSome dense vector (dim significantly less then the image) we could use in other applications\n\nAssess similarity of images (almost same as 1.)\n\nSome function \\(f\\) such that similarity would presumably be high for the same digits, but written differently and somewhat high for digits that are just written similarly to each other.\n\nDiscover some underlying structure in the data (e.g., clusters) (almost same as 1.)\n\nFor example, being able to tell what digit is on the image.\n\nGuided generation.\n\nBeing able to generate image of a certain digit, or a digit of a certain style."
  },
  {
    "objectID": "posts/2025-10-02-bernoulli-random-walks/index.html",
    "href": "posts/2025-10-02-bernoulli-random-walks/index.html",
    "title": "Bernoulli Random Walks",
    "section": "",
    "text": "Here is one of the most common misconceptions that I see people have when thinking about probability, in particular repeated coin tosses:\n“Tossing a fair coin n times will give you roughly the same number of heads and tails.”\nTo formalize, let’s say you have iid \\(X_1, \\dots, X_n\\), where \\(X_i \\sim Bi(p)\\), that is, \\(X_i = \\begin{cases} 1, & p \\\\ -1, & 1-p \\end{cases}\\).\nLets define the walk \\(S_t=\\sum_{i}^t X_i\\).\nthen, the assertion is \\(P(S_n = 0) \\text{ is high}\\)\nIn fact, the opposite is true:\n\\(P(S_n=0) = \\frac{\\binom{n}{n/2}}{2^n} \\sim \\sqrt{\\frac{2}{\\pi n}} \\to 0\\) as \\(n \\to \\infty\\)\nFor example, \\(n=10: 0.246, n=100: \\sim 0.08, n=1000: \\sim 0.025\\)\nWe test our intuition today with a little experiment.\nWe are going to be looking at trajectories, where each point is \\((t, S_t)\\).\nIf \\(S_t\\) is zero, then we have the same number of tails and heads at time \\(t\\).\nOur intuition tells us:\nHere is somewhat our expectations summarized:\nText(0, 0.5, 'S_t')\nAnd here are the actual trajectories:\nAnd here are the distributions of some core statistics:\nWhat this shows:"
  },
  {
    "objectID": "posts/2025-10-02-bernoulli-random-walks/index.html#so-why-did-our-intuition-fail-us",
    "href": "posts/2025-10-02-bernoulli-random-walks/index.html#so-why-did-our-intuition-fail-us",
    "title": "Bernoulli Random Walks",
    "section": "So why did our intuition fail us?",
    "text": "So why did our intuition fail us?\nThe random walk is fair in the sense that \\(E[S_n]=0\\) and the ditribution of \\(S_n\\) is symmetric. Some might find it tempting to assume that it implies “it should stay near 0”.\nSome people might misinterpret the law of large number and think that $S_n $ as \\(n \\rightarrow \\inf\\), while what it actually gives us is \\(S_n/n \\rightarrow 0\\), which basically says that the average step tends to zero.\n\\(0\\) is indeed the most like value that \\(S_n\\) takes (the mode), but the problem is the distribution of \\(S_n\\) “spread out” as \\(n\\) grows:\n\\[Var(S_n) = n\\]\nSo the trajectories will lie in a \\(\\pm\\sqrt{t}\\) band.\nFrom CLT, \\(\\frac{S_t}{\\sqrt{t}} \\rightarrow \\mathcal{N}(0, 1)\\), so \\(P(|S_t|\\le 2 \\sqrt{t}) \\sim 95\\%\\)\n\n\nText(0, 0.5, 'S_t')"
  },
  {
    "objectID": "myblog.html",
    "href": "myblog.html",
    "title": "myblog",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "myblog.html#quarto",
    "href": "myblog.html#quarto",
    "title": "myblog",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "posts/2025-09-25-test/index.html",
    "href": "posts/2025-09-25-test/index.html",
    "title": "Test: Math + Python (Notebook)",
    "section": "",
    "text": "This post demonstrates math and executable Python in an .ipynb."
  },
  {
    "objectID": "posts/2025-09-25-test/index.html#math",
    "href": "posts/2025-09-25-test/index.html#math",
    "title": "Test: Math + Python (Notebook)",
    "section": "Math",
    "text": "Math\nThe quadratic formula:\n\\[x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\]\nA short inline sum: \\(\\sum_{i=1}^n i = \\frac{n(n+1)}{2}\\).\n\nimport math\n\ndef quadratic_roots(a, b, c):\n    disc = b*b - 4*a*c\n    return ((-b - math.sqrt(disc)) / (2*a), (-b + math.sqrt(disc)) / (2*a))\n\nquadratic_roots(1, -3, 2)\n\n(1.0, 2.0)"
  },
  {
    "objectID": "posts/2025-09-25-test/index.html#plot-example",
    "href": "posts/2025-09-25-test/index.html#plot-example",
    "title": "Test: Math + Python (Notebook)",
    "section": "Plot example",
    "text": "Plot example\nA quick Matplotlib plot.\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-3, 3, 200)\ny = x**2 - 3*x + 2\n\nplt.plot(x, y)\nplt.title(\"y = x^2 - 3x + 2\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/2025-09-25-test/index.html#interactive-plot-example",
    "href": "posts/2025-09-25-test/index.html#interactive-plot-example",
    "title": "Test: Math + Python (Notebook)",
    "section": "Interactive plot example",
    "text": "Interactive plot example\n\nimport plotly.express as px\n\ndf = px.data.iris()\nfig = px.scatter(\n    df, x=\"sepal_width\", y=\"sepal_length\",\n    color=\"species\", title=\"Interactive Iris Scatter\", height=420\n)\nfig  # returning the figure renders it interactively"
  },
  {
    "objectID": "posts/2025-09-25-test/index.html#interactive-plot-with-hide-input",
    "href": "posts/2025-09-25-test/index.html#interactive-plot-with-hide-input",
    "title": "Test: Math + Python (Notebook)",
    "section": "Interactive plot with hide input",
    "text": "Interactive plot with hide input"
  },
  {
    "objectID": "posts/2025-09-25-test/index.html#with-slider",
    "href": "posts/2025-09-25-test/index.html#with-slider",
    "title": "Test: Math + Python (Notebook)",
    "section": "With-slider",
    "text": "With-slider\n\nimport plotly.express as px\ndf = px.data.gapminder()\nfig = px.scatter(\n    df, x=\"gdpPercap\", y=\"lifeExp\",\n    animation_frame=\"year\", animation_group=\"country\",\n    size=\"pop\", color=\"continent\", hover_name=\"country\",\n    log_x=True, size_max=45, range_x=[100, 100000], range_y=[25, 90],\n    title=\"Gapminder (use slider)\"\n)\nfig"
  }
]